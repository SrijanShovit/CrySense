{"metadata":{"colab":{"provenance":[],"gpuType":"T4","collapsed_sections":["99FRzLHUzvjR","Yd64T_vhz0qK","ZbKrHG-CqOsv"]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7288843,"sourceType":"datasetVersion","datasetId":4227039}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <span style=\"font-family:cursive;text-align:center\">‚¨áÔ∏è Import Libraries</span>","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport librosa\nimport librosa.display\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import normalize\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.model_selection import train_test_split\nimport tensorflow\nimport numpy as np\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Input, Flatten, Dense, Dropout, Resizing, Normalization\nfrom tensorflow.keras.optimizers import AdamW\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical\n\nfrom tensorflow.keras.layers import LSTM, Dense","metadata":{"id":"Pti8RhxqMjTe","execution":{"iopub.status.busy":"2023-12-27T09:15:08.843877Z","iopub.execute_input":"2023-12-27T09:15:08.844326Z","iopub.status.idle":"2023-12-27T09:15:21.943194Z","shell.execute_reply.started":"2023-12-27T09:15:08.844299Z","shell.execute_reply":"2023-12-27T09:15:21.942168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">\n&nbsp;<b>The function create_spectrogram takes an input audio file path and generates a spectrogram image, saving it to the specified image file path. It utilizes the librosa library to load the audio, compute its mel spectrogram, and convert the power spectrogram to a decibel scale for better visualization.</b><br>\n<br>\n&nbsp;<b>The create_pngs_from_wavs function automates the process for multiple audio files within a given directory. It converts all .wav files in the specified input directory into spectrogram images, saving the resulting images to the designated output directory. This function makes use of the create_spectrogram function internally to generate the spectrogram images.","metadata":{}},{"cell_type":"markdown","source":"# <span style=\"font-family:cursive;text-align:center\">üìä Data Processing and Training set generation</span>","metadata":{}},{"cell_type":"code","source":"def create_spectrogram(audio_file, image_file):\n    fig = plt.figure()\n    ax = fig.add_subplot(1, 1, 1)\n    fig.subplots_adjust(left=0, right=1, bottom=0, top=1)\n\n    y, sr = librosa.load(audio_file)\n    ms = librosa.feature.melspectrogram(y=y, sr=sr)\n    log_ms = librosa.power_to_db(ms, ref=np.max)\n    librosa.display.specshow(log_ms, sr=sr)\n\n    fig.savefig(image_file)\n    plt.close(fig)\n\ndef create_pngs_from_wavs(input_path, output_path):\n    if not os.path.exists(output_path):\n        os.makedirs(output_path)\n\n    dir = os.listdir(input_path)\n\n    for i, file in enumerate(dir):\n        input_file = os.path.join(input_path, file)\n        output_file = os.path.join(output_path, file.replace('.wav', '.png'))\n        create_spectrogram(input_file, output_file)","metadata":{"id":"Vb4grOJHnfgu","execution":{"iopub.status.busy":"2023-12-27T09:19:09.820292Z","iopub.execute_input":"2023-12-27T09:19:09.820658Z","iopub.status.idle":"2023-12-27T09:19:09.830121Z","shell.execute_reply.started":"2023-12-27T09:19:09.820632Z","shell.execute_reply":"2023-12-27T09:19:09.829078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"create_pngs_from_wavs('/kaggle/input/aud-data/aug-dataset1/belly_pain', '/kaggle/working/belly_pain')\ncreate_pngs_from_wavs('/kaggle/input/aud-data/aug-dataset1/burping', '/kaggle/working/burping')\ncreate_pngs_from_wavs('/kaggle/input/aud-data/aug-dataset1/discomfort', '/kaggle/working/discomfort')\ncreate_pngs_from_wavs('/kaggle/input/aud-data/aug-dataset1/hungry', '/kaggle/working/hungry')\ncreate_pngs_from_wavs('/kaggle/input/aud-data/aug-dataset1/tired', '/kaggle/working/tired')","metadata":{"id":"99ri3l3-noK7","execution":{"iopub.status.busy":"2023-12-27T09:19:13.513942Z","iopub.execute_input":"2023-12-27T09:19:13.514306Z","iopub.status.idle":"2023-12-27T09:20:32.222329Z","shell.execute_reply.started":"2023-12-27T09:19:13.514279Z","shell.execute_reply":"2023-12-27T09:20:32.220928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">\n    &nbsp;<b>Loading the images from the path</b><br>\n</div>","metadata":{}},{"cell_type":"code","source":"x = []\ny = []\n\nfrom keras.preprocessing import image\n\ndef load_images_from_path(path, label):\n    images = []\n    labels = []\n\n    for file in os.listdir(path):\n        images.append(image.img_to_array(image.load_img(os.path.join(path, file), target_size=(224, 224, 3))))\n        labels.append((label))\n\n    return images, labels","metadata":{"id":"e6VCZhdinzm4","execution":{"iopub.status.busy":"2023-12-27T09:21:07.103401Z","iopub.execute_input":"2023-12-27T09:21:07.103754Z","iopub.status.idle":"2023-12-27T09:21:07.110179Z","shell.execute_reply.started":"2023-12-27T09:21:07.103725Z","shell.execute_reply":"2023-12-27T09:21:07.109229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">\n     &nbsp;<b>The function takes in source patterns and their corresponding destination paths as input. If the destination directory doesn't exist, it creates the directory and proceeds with moving the files. It utilizes the glob module to identify the files based on the specified patterns and shutil for the file movement.\n</b><br>\n    <br>\n    &nbsp;<b>With this we create the training dataset and leave a file for each type for testing</b><br>\n</div>","metadata":{}},{"cell_type":"code","source":"import glob\nimport shutil\n\ndef move_files(source_pattern, destination_path):\n    if not os.path.exists(destination_path):\n        os.makedirs(destination_path)\n        print(f\"Directory '{destination_path}' created successfully.\")\n    else:\n        print(f\"Directory '{destination_path}' already exists.\")\n\n    files_to_move = glob.glob(source_pattern)\n    for file_path in files_to_move[:-1]:\n        shutil.move(file_path, destination_path)\n\n# Define your directories and source patterns\ndirectories = {\n    '/kaggle/working/belly_pain_train/': '/kaggle/working/belly_pain/*.png',\n    '/kaggle/working/burping_train/': '/kaggle/working/burping/*.png',\n    '/kaggle/working/discomfort_train/': '/kaggle/working/discomfort/*.png',\n    '/kaggle/working/hungry_train/': '/kaggle/working/hungry/*.png',\n    '/kaggle/working/tired_train/': '/kaggle/working/tired/*.png'\n}\n\n# Loop through the directories and move the files\nfor directory, source_pattern in directories.items():\n    move_files(source_pattern, directory)","metadata":{"id":"IqABUS4SubDU","outputId":"03a5bcf4-4140-4d80-9ffd-447eee31920c","execution":{"iopub.status.busy":"2023-12-27T09:21:08.867021Z","iopub.execute_input":"2023-12-27T09:21:08.867364Z","iopub.status.idle":"2023-12-27T09:21:08.898375Z","shell.execute_reply.started":"2023-12-27T09:21:08.867340Z","shell.execute_reply":"2023-12-27T09:21:08.897470Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images, labels = load_images_from_path('/kaggle/working/belly_pain_train', 0)\n\nx += images\ny += labels\n\nimages, labels = load_images_from_path('/kaggle/working/burping_train', 1)\n\nx += images\ny += labels\n\nimages, labels = load_images_from_path('/kaggle/working/discomfort_train', 2)\n\nx += images\ny += labels\n\nimages, labels = load_images_from_path('/kaggle/working/hungry_train', 3)\n\nx += images\ny += labels\n\nimages, labels = load_images_from_path('/kaggle/working/tired_train', 4)\n\nx += images\ny += labels","metadata":{"id":"j3lWFbHdn2ca","execution":{"iopub.status.busy":"2023-12-27T09:21:11.435658Z","iopub.execute_input":"2023-12-27T09:21:11.436080Z","iopub.status.idle":"2023-12-27T09:21:15.150023Z","shell.execute_reply.started":"2023-12-27T09:21:11.436047Z","shell.execute_reply":"2023-12-27T09:21:15.149264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\nx_train_norm = np.array(x_train) / 255\nx_test_norm = np.array(x_test) / 255\nx_val_norm = np.array(x_val) / 255\ny_train_encoded = to_categorical(y_train)\ny_test_encoded = to_categorical(y_test)\ny_val_encoded = to_categorical(y_val, num_classes=5)\nclass_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\nclass_weight_dict = dict(enumerate(class_weights))\n","metadata":{"id":"wtkszY8-n3-I","execution":{"iopub.status.busy":"2023-12-27T09:26:16.096503Z","iopub.execute_input":"2023-12-27T09:26:16.097105Z","iopub.status.idle":"2023-12-27T09:26:16.200095Z","shell.execute_reply.started":"2023-12-27T09:26:16.097077Z","shell.execute_reply":"2023-12-27T09:26:16.199283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span style=\"font-family:cursive;text-align:center\">‚öôÔ∏è Model Training</span>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; border:#DEB887 solid; padding: 15px; background-color: #FFFAF0; font-size:100%; text-align:left\">\n<h3 align=\"left\"><font color='#DEB887'>üí° Model Architecture:</font></h3>\n    \n1. **Input Layer**: Accepts input images of shape (224, 224, 3), which corresponds to images with a height and width of 224 pixels and three color channels (RGB).\n2. **Resizing Layer: Resizes the input images to a smaller size of (64, 64). This reduction in image dimensions may help speed up training.\n3. **Normalization Layer**: Normalizes pixel values to have zero mean and unit variance, which aids in stabilizing and speeding up the training process.\n4. **Convolutional Layers**: Utilizes two convolutional layers:\n     - The first convolutional layer has 64 filters, a kernel size of 3x3, and ReLU activation.\n     - The second convolutional layer has 128 filters, a kernel size of 3x3, and ReLU activation.\n7. **MaxPooling Layer**: Performs max pooling with a pool size of 2x2, reducing the spatial dimensions of the feature maps.\n8. **Dropout Layer**: Introduces a dropout rate of 20% to prevent overfitting by randomly deactivating a fraction of neurons during training.\n9. **Flatten Layer**: Flattens the 2D feature maps into a 1D vector to prepare for the fully connected layers.\n10. **RandomFourierFeatures Layer**: Incorporates random Fourier features with 5 components, which can approximate non-linear mappings efficiently for the data.\n11. **Compilation**: Compiles the model using the AdamW optimizer with a learning rate of 0.01, categorical cross-entropy loss function (suitable for multi-class classification), and accuracy as the evaluation metric.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D,Flatten, Dense, Dropout,Normalization,Resizing,InputLayer\nfrom tensorflow.keras.layers.experimental import RandomFourierFeatures\nfrom tensorflow.keras.optimizers import Adam,Adafactor,AdamW,Lion\nfrom tensorflow.keras.optimizers.experimental import Adadelta,Adagrad,Adamax,RMSprop,SGD,Nadam,Ftrl\n\n\n# Load pre-trained VGG16 model without top layers\nbase_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n\n# Freeze the convolutional layers of VGG16\nfor layer in base_model.layers:\n    layer.trainable = False\n\n# Create a new Sequential model\nmodel = Sequential()\nmodel.add(Input(shape=(224, 224, 3)))\nmodel.add(Resizing(224, 224))\nmodel.add(Normalization())\nmodel.add(base_model)\nmodel.add(Flatten())\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(5, activation='softmax'))\n# Learning rate schedule for AdamW optimizer\ninitial_learning_rate = 0.001\nlr_schedule = ExponentialDecay(\n    initial_learning_rate, decay_steps=10000, decay_rate=0.9, staircase=True\n)\n\n# Compile the model\nmodel.compile(optimizer=AdamW(learning_rate=lr_schedule), loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()\n","metadata":{"id":"oa1b9vgEn6kG","outputId":"9e90d311-00b1-420e-ca68-5998e7dea087","execution":{"iopub.status.busy":"2023-12-27T09:26:37.012656Z","iopub.execute_input":"2023-12-27T09:26:37.013376Z","iopub.status.idle":"2023-12-27T09:26:37.503210Z","shell.execute_reply.started":"2023-12-27T09:26:37.013345Z","shell.execute_reply":"2023-12-27T09:26:37.502349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nhist = model.fit(x_train_norm, y_train_encoded, batch_size=32,\n    steps_per_epoch=len(x_train_norm) / 32,\n    epochs=20,\n    validation_data=(x_val_norm, y_val_encoded),\n    class_weight=class_weight_dict\n    )\n# Evaluate on test set\ntest_loss, test_acc = model.evaluate(x_test_norm, y_test_encoded)\nprint(f'Test accuracy: {test_acc}')","metadata":{"id":"AB8UY1DgseoL","outputId":"c1694b58-9705-4b2e-9618-745560f9c867","execution":{"iopub.status.busy":"2023-12-27T09:28:22.802384Z","iopub.execute_input":"2023-12-27T09:28:22.802749Z","iopub.status.idle":"2023-12-27T09:28:39.423787Z","shell.execute_reply.started":"2023-12-27T09:28:22.802718Z","shell.execute_reply":"2023-12-27T09:28:39.422873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc = hist.history['accuracy']\nval_acc = hist.history['val_accuracy']\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, '-', label='Training Accuracy')\nplt.plot(epochs, val_acc, ':', label='Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.plot()\nplt.show()","metadata":{"id":"6lgzf5RIn-m-","outputId":"6ebd621c-9c91-4a5d-ec3b-3989e7badaf9","execution":{"iopub.status.busy":"2023-12-27T09:28:44.840458Z","iopub.execute_input":"2023-12-27T09:28:44.840797Z","iopub.status.idle":"2023-12-27T09:28:45.180066Z","shell.execute_reply.started":"2023-12-27T09:28:44.840771Z","shell.execute_reply":"2023-12-27T09:28:45.179143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span style=\"font-family:cursive;text-align:center\">üß™ Testing</span>","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">\n    &nbsp;<b>For predicting on single tets files this code will be used. For each image file path obtained, the code performs the following operations:\n\n- Loads the image using image.load_img() from the Keras preprocessing module, resizing it to the required dimensions of 224x224 pixels.\n- Converts the image to a format compatible with the model for prediction (image.img_to_array() and np.expand_dims()).\n- Utilizes the model for prediction on the image\n- Retrieves class labels corresponding to the categories ('belly pain', 'burping', 'discomfort', 'hungry', 'tired').\n- Prints the original file path and its predicted category, appending this information to the results list.</b>\n</div>","metadata":{}},{"cell_type":"code","source":"import glob\nimport os\n\ndef get_png_files(directory):\n    folders = ['belly_pain', 'burping', 'discomfort', 'hungry', 'tired'] \n    png_files = []\n\n    for folder_name in folders:\n        folder_path = os.path.join(directory, folder_name)\n        if os.path.exists(folder_path):\n            png_files.extend(glob.glob(os.path.join(folder_path, '*.png')))\n        else:\n            print(f\"Folder '{folder_name}' does not exist in '{directory}'.\")\n\n    return png_files\n\ndirectory_path = '/kaggle/working/' \npng_files_list = get_png_files(directory_path)\n\nresults = []\nfor file_path in png_files_list:\n    x = image.load_img(file_path, target_size=(224, 224))\n\n    x = image.img_to_array(x)\n    x = np.expand_dims(x, axis=0)\n\n    y = model.predict(x)\n\n    class_labels = ['belly pain','burping','discomfort','hungry','tired']\n\n    # for i, label in enumerate(class_labels):\n    #     print(f'{label}: {y[0][i]}')\n\n    results.append(f\"Original:{file_path.split('/')[3]} Predicted: {class_labels[np.argmax(y)]}\")\n    \nprint('\\n')\n\nfor i in results:\n    print(i)\n    print('\\n')","metadata":{"execution":{"iopub.status.busy":"2023-12-27T09:29:04.059133Z","iopub.execute_input":"2023-12-27T09:29:04.059847Z","iopub.status.idle":"2023-12-27T09:29:04.801211Z","shell.execute_reply.started":"2023-12-27T09:29:04.059814Z","shell.execute_reply":"2023-12-27T09:29:04.800316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}